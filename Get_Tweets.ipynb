{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Get_Tweets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPp62HEDX5rDyz5hHd0OLQx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2uOVedzIavT"
      },
      "source": [
        "# Gathering Tweets from Twitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V08Pi79e_mut"
      },
      "source": [
        "This notebook is for gathering tweets from Twitter about a specific word query and date. All data gathered cleaned with the help of NLP techniques and in the end dataframe converted into csv format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4TeVBKEIr-o"
      },
      "source": [
        "!pip install tweepy\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAEcaDl6IX5n"
      },
      "source": [
        "from tqdm import tqdm, notebook\n",
        "import tweepy as tw\n",
        "import textblob\n",
        "import os\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.width', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M-xYoznI-Xk"
      },
      "source": [
        "Twitter API authentication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMIjh5RXI7Fj"
      },
      "source": [
        "api_key = \"\"\n",
        "api_secret_key = \"\"\n",
        "access_token = \"\"\n",
        "access_token_secret = \"\"\n",
        "Bearer_token: \"\"\n",
        "\n",
        "auth = tw.OAuthHandler(api_key, api_secret_key)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "try:\n",
        "    api.verify_credentials()\n",
        "    print(\"Authentication OK\")\n",
        "except:\n",
        "    print(\"Error during authentication\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4XvrSNiJNSz"
      },
      "source": [
        "# Tweets query\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE-_4S61zJ1h"
      },
      "source": [
        "Searching and obtaining tweets with a specific query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7MJZO5_vlel"
      },
      "source": [
        "# query attributes\n",
        "search_words = \"#covid19 -filter:retweets\"\n",
        "date_since = \"2021-01-01\"\n",
        "\n",
        "# Collect tweets\n",
        "tweets = tw.Cursor(api.search,\n",
        "              q=search_words,\n",
        "              lang=\"en\",\n",
        "              since=date_since).items(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO_a3FzQvvUj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "569ab950-0ef3-400a-b375-99f6ba699d18"
      },
      "source": [
        "tweets_copy = []\n",
        "for tweet in tqdm(tweets):\n",
        "     tweets_copy.append(tweet)\n",
        "     \n",
        "print(f\"new tweets retrieved: {len(tweets_copy)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000it [00:35, 28.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new tweets retrieved: 1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZDXALzZzHdh"
      },
      "source": [
        "Converting the dataset into pandas dataframe format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeXykkO0zB6Z"
      },
      "source": [
        "tweets_df = pd.DataFrame()\n",
        "for tweet in tqdm(tweets_copy):\n",
        "    hashtags = []\n",
        "    try:\n",
        "        for hashtag in tweet.entities[\"hashtags\"]:\n",
        "            hashtags.append(hashtag[\"text\"])\n",
        "        text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
        "    except:\n",
        "        pass\n",
        "    tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name, \n",
        "                                               'user_location': tweet.user.location,\\\n",
        "                                               'user_description': tweet.user.description,\n",
        "                                               'user_created': tweet.user.created_at,\n",
        "                                               'user_followers': tweet.user.followers_count,\n",
        "                                               'user_friends': tweet.user.friends_count,\n",
        "                                               'user_favourites': tweet.user.favourites_count,\n",
        "                                               'user_verified': tweet.user.verified,\n",
        "                                               'date': tweet.created_at,\n",
        "                                               'text': text, \n",
        "                                               'hashtags': [hashtags if hashtags else None],\n",
        "                                               'source': tweet.source,\n",
        "                                               'is_retweet': tweet.retweeted}, index=[0]))\n",
        "    \n",
        "    \n",
        "tweets_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XSvR_OG23pb"
      },
      "source": [
        "Clear the duplicates if there are any"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fcxa9VuO0K_N",
        "outputId": "8c9414f2-e22b-4245-84b1-0b30d25bd455"
      },
      "source": [
        "tweets_df.drop_duplicates(subset = [\"user_name\", \"date\", \"text\"], inplace=True)\n",
        "print(f\"all tweets: {tweets_df.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all tweets: (1000, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SewJLkA2lqZ"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQUJQvVxApsV"
      },
      "source": [
        "tweets_df=tweets_df[['text']]\n",
        "tweets_df['text']=tweets_df['text'].str.lower() # all tweets cconverted to the lowercase for working better\n",
        "print(tweets_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiPc4WXN1dk3"
      },
      "source": [
        "english_punctuations = string.punctuation\n",
        "punctuations_list = english_punctuations\n",
        "def cleaning_punctuations(text):\n",
        "    translator = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(translator)\n",
        "tweets_df['text']=tweets_df['text'].apply(lambda x: cleaning_punctuations(x))\n",
        "tweets_df['text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCZoNfSZ1rE9"
      },
      "source": [
        "def cleaning_email(data):\n",
        "    return re.sub('@[^\\s]+', ' ', data)\n",
        "\n",
        "tweets_df['text']= tweets_df['text'].apply(lambda x: cleaning_email(x))\n",
        "\n",
        "def cleaning_URLs(data):\n",
        "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',data)\n",
        "\n",
        "tweets_df['text'] = tweets_df['text'].apply(lambda x: cleaning_URLs(x))\n",
        "\n",
        "\n",
        "def cleaning_numbers(data):\n",
        "    return re.sub('[0-9]+', '', data)\n",
        "\n",
        "tweets_df['text'] = tweets_df['text'].apply(lambda x: cleaning_numbers(x))\n",
        "tweets_df['text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeHZVCAx1-pZ"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def cleaning_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
        "tweets_df['text'] = tweets_df['text'].apply(lambda text: cleaning_stopwords(text))\n",
        "tweets_df['text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fG56UJ82IgE"
      },
      "source": [
        "# Stemming\n",
        "st = nltk.PorterStemmer()\n",
        "def stemming_on_text(data):\n",
        "    text = [st.stem(word) for word in data]\n",
        "    return data\n",
        "\n",
        "tweets_df['text']= tweets_df['text'].apply(lambda x: stemming_on_text(x))\n",
        "\n",
        "# Applying Lemmatizer\n",
        "lm = nltk.WordNetLemmatizer()\n",
        "def lemmatizer_on_text(data):\n",
        "    text = [lm.lemmatize(word) for word in data]\n",
        "    return data\n",
        "\n",
        "tweets_df['text'] = tweets_df['text'].apply(lambda x: lemmatizer_on_text(x))\n",
        "tweets_df['text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwhSy4Zm3Bw_"
      },
      "source": [
        "Export the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc0WOb--28Tc"
      },
      "source": [
        "tweets_df.to_csv(\"test.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}