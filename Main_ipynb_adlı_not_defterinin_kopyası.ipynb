{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main.ipynb adlı not defterinin kopyası",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO3jhQnIvET0s+kWlMNkHeD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DogaSahin/Twitter_Sentiment_Analysis/blob/main/Main_ipynb_adl%C4%B1_not_defterinin_kopyas%C4%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhorl27WQmwS"
      },
      "source": [
        "# Training Model & Preparing a proper train/test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n62QZUVBApZc"
      },
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hzkcukQBXvG"
      },
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO7-BpGYY4-g",
        "outputId": "699acc4f-cb26-4be6-9975-2f49b4a78e0f"
      },
      "source": [
        "!kaggle datasets download -d kazanova/sentiment140\n",
        "! unzip sentiment140.zip -d sample_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentiment140.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  sentiment140.zip\n",
            "  inflating: sample_data/training.1600000.processed.noemoticon.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx4CCPwoQ1Zs"
      },
      "source": [
        "Importing necesary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHAw4aT9XNPQ",
        "outputId": "c1af1718-ac77-4e50-84e3-2deafc9a5e70"
      },
      "source": [
        "!pip install nltk\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrp4SVxpwfpO"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer \n",
        "from sklearn.model_selection import train_test_split\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from matplotlib import rcParams\n",
        "from collections import Counter\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import re\n",
        "import string\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-EfI7qIieGs",
        "outputId": "75ef1948-3d8c-4b37-8f4e-e073e280f3bc"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N76MABuDQ4zy"
      },
      "source": [
        "Getting a labeled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnNSV_Sx4ZN-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "f98cb13c-b673-4bed-e045-2cd379875690"
      },
      "source": [
        "data = pd.read_csv(\"sample_data/training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", engine=\"python\")\n",
        "data.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]\n",
        "data.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>time</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>username</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811372</td>\n",
              "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>joy_wolf</td>\n",
              "      <td>@Kwesidei not the whole crew</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  ...                                               text\n",
              "0      0  ...  is upset that he can't update his Facebook by ...\n",
              "1      0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "2      0  ...    my whole body feels itchy and like its on fire \n",
              "3      0  ...  @nationwideclass no, it's not behaving at all....\n",
              "4      0  ...                      @Kwesidei not the whole crew \n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL03GC7b4tQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa19cf3-5b3c-4231-dba0-50ecbf105f35"
      },
      "source": [
        "data.columns\n",
        "print(len(data))\n",
        "data.shape\n",
        "data.info()\n",
        "data.dtypes\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1599999\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1599999 entries, 0 to 1599998\n",
            "Data columns (total 6 columns):\n",
            " #   Column    Non-Null Count    Dtype \n",
            "---  ------    --------------    ----- \n",
            " 0   label     1599999 non-null  int64 \n",
            " 1   time      1599999 non-null  int64 \n",
            " 2   date      1599999 non-null  object\n",
            " 3   query     1599999 non-null  object\n",
            " 4   username  1599999 non-null  object\n",
            " 5   text      1599999 non-null  object\n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 73.2+ MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label        int64\n",
              "time         int64\n",
              "date        object\n",
              "query       object\n",
              "username    object\n",
              "text        object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMeLYXUJP8A0"
      },
      "source": [
        "Making a more meaningfull dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8C8lKYC5JFV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "ea37ffc3-ef65-4c04-f96d-0c8453be154a"
      },
      "source": [
        "# Selecting the text and label coloumn Assigning 1 to Positive sentment 4\n",
        "\n",
        "data=data[['text','label']]\n",
        "data['label'][data['label']==4]=1\n",
        "\n",
        "# Separating positive and negative tweets\n",
        "positive_data = data[data['label'] == 1]\n",
        "negative_data = data[data['label'] == 0]\n",
        "\n",
        "# taking half of the dataset for try\n",
        "positive_data = positive_data.iloc[:int(80000)]\n",
        "negative_data = negative_data.iloc[:int(80000)]\n",
        "\n",
        "# Combining positive and negative tweets & making all tweets lowercase so we can work better\n",
        "data = pd.concat([positive_data, negative_data])\n",
        "data['text']=data['text'].str.lower()\n",
        "\n",
        "data['text'].head()\n",
        "data.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>79995</th>\n",
              "      <td>@itsaroy they hurt my feelings when they don't...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79996</th>\n",
              "      <td>excited that josh will be back in erie in an h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79997</th>\n",
              "      <td>plans of going to the club have been dashed. n...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79998</th>\n",
              "      <td>i hate car washing</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79999</th>\n",
              "      <td>cannot actually keep my eyes open.., i feel li...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  label\n",
              "79995  @itsaroy they hurt my feelings when they don't...      0\n",
              "79996  excited that josh will be back in erie in an h...      0\n",
              "79997  plans of going to the club have been dashed. n...      0\n",
              "79998                                i hate car washing       0\n",
              "79999  cannot actually keep my eyes open.., i feel li...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgRfQNIrRC0r"
      },
      "source": [
        "Cleaning and removing punctuations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9LT3HheREq5",
        "outputId": "2716f5e3-43ee-46ed-df01-c7eba8378fd6"
      },
      "source": [
        "english_punctuations = string.punctuation\n",
        "punctuations_list = english_punctuations\n",
        "def cleaning_punctuations(text):\n",
        "    translator = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(translator)\n",
        "data['text']= data['text'].apply(lambda x: cleaning_punctuations(x))\n",
        "data['text'].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "799999            i love health4uandpets u guys r the best \n",
              "800000    im meeting up with one of my besties tonight c...\n",
              "800001    darealsunisakim thanks for the twitter add sun...\n",
              "800002    being sick can be really cheap when it hurts t...\n",
              "800003       lovesbrooklyn2 he has that effect on everyone \n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgpfl4WZSLX6"
      },
      "source": [
        "Removing email & URL's\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LrEUMXXSQiF",
        "outputId": "ba32f09b-49f0-4756-8d59-983e7caae78d"
      },
      "source": [
        "def cleaning_email(data):\n",
        "    return re.sub('@[^\\s]+', ' ', data)\n",
        "\n",
        "data['text']= data['text'].apply(lambda x: cleaning_email(x))\n",
        "\n",
        "def cleaning_URLs(data):\n",
        "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',data)\n",
        "\n",
        "data['text'] = data['text'].apply(lambda x: cleaning_URLs(x))\n",
        "data['text'].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "799999            i love health4uandpets u guys r the best \n",
              "800000    im meeting up with one of my besties tonight c...\n",
              "800001    darealsunisakim thanks for the twitter add sun...\n",
              "800002    being sick can be really cheap when it hurts t...\n",
              "800003       lovesbrooklyn2 he has that effect on everyone \n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cersTlNOTK4d"
      },
      "source": [
        "Cleaning and removing Numeric numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fzys2jenTLsG",
        "outputId": "c4da323a-7c43-43d6-bd51-a92749a45c2a"
      },
      "source": [
        "def cleaning_numbers(data):\n",
        "    return re.sub('[0-9]+', '', data)\n",
        "\n",
        "data['text'] = data['text'].apply(lambda x: cleaning_numbers(x))\n",
        "data['text'].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "799999             i love healthuandpets u guys r the best \n",
              "800000    im meeting up with one of my besties tonight c...\n",
              "800001    darealsunisakim thanks for the twitter add sun...\n",
              "800002    being sick can be really cheap when it hurts t...\n",
              "800003        lovesbrooklyn he has that effect on everyone \n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goif66ynQIfN"
      },
      "source": [
        "Cleaning stopwords from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgIp8n91U3dR",
        "outputId": "75c14159-4fc4-41ec-ac29-d3ae4f9963c3"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "{'their', \"you're\", 'than', \"she's\", 'few', 'after', \"couldn't\", 'here', 'isn', 'through', 'each', \"don't\", 'very', 'mustn', 'nor', 'only', 'too', 'ours', \"you'll\", \"you've\", 'an', 'being', 'doing', 'this', 'who', 'where', \"isn't\", 'been', 'are', 'in', \"needn't\", \"you'd\", 'themselves', 'hadn', 'had', 't', \"it's\", 'now', 'before', \"weren't\", 'the', 'it', 'am', 'of', 'out', 'but', 'm', 're', 'we', 'mightn', 'so', 'up', 'wouldn', 'were', 'll', 'didn', 'at', 'again', 'hasn', 'does', 'd', 'be', 'until', 'shan', \"that'll\", 'over', \"won't\", 'against', 'not', 'his', 'own', 'wasn', 'he', 'all', 'yours', 'has', 'just', 'a', 'into', 'down', 'if', 'having', \"shouldn't\", 'herself', 'that', 'yourselves', 's', 'most', 'its', 'himself', 'itself', 'about', 'was', 'above', 'once', 'can', 'needn', \"mightn't\", 'myself', 'yourself', 've', \"doesn't\", 'when', 'ourselves', 'do', 'aren', 'ain', 'with', \"should've\", 'i', \"didn't\", 'from', \"wouldn't\", 'doesn', 'because', 'ma', \"wasn't\", 'shouldn', 'my', 'theirs', 'under', 'there', 'y', \"mustn't\", 'which', 'or', 'below', 'and', 'by', \"hasn't\", 'should', 'them', 'have', 'why', 'weren', 'whom', 'him', 'some', \"haven't\", 'what', 'other', 'such', 'during', \"aren't\", 'between', 'haven', 'no', 'hers', 'as', 'is', 'on', 'both', 'don', 'for', 'our', 'o', 'you', 'then', 'won', 'couldn', 'will', 'me', \"hadn't\", 'same', 'she', 'further', 'her', 'how', 'these', 'off', 'any', 'while', 'to', \"shan't\", 'your', 'more', 'did', 'they', 'those'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQWmixcYP6Ii",
        "outputId": "72bcda55-a47a-4eec-e269-d5a7cb1e695a"
      },
      "source": [
        "def cleaning_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
        "data['text'] = data['text'].apply(lambda text: cleaning_stopwords(text))\n",
        "data['text'].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "799999                    love healthuandpets u guys r best\n",
              "800000    im meeting one besties tonight cant wait girl ...\n",
              "800001    darealsunisakim thanks twitter add sunisa got ...\n",
              "800002    sick really cheap hurts much eat real food plu...\n",
              "800003                        lovesbrooklyn effect everyone\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDhzhCg3aY6V"
      },
      "source": [
        "tokenization of tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70Xw1WE2aZ9H",
        "outputId": "353bcc8a-2ea4-4d3d-8053-e25a3175b437"
      },
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "data['text'] = data['text'].apply(tokenizer.tokenize)\n",
        "data['text'].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "799999             [love, healthuandpets, u, guys, r, best]\n",
              "800000    [im, meeting, one, besties, tonight, cant, wai...\n",
              "800001    [darealsunisakim, thanks, twitter, add, sunisa...\n",
              "800002    [sick, really, cheap, hurts, much, eat, real, ...\n",
              "800003                    [lovesbrooklyn, effect, everyone]\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXLpadr4aebU"
      },
      "source": [
        "Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIFhcOGcafKa",
        "outputId": "a47cd1b8-1505-441d-ddc3-f4d25c091c0a"
      },
      "source": [
        "st = nltk.PorterStemmer()\n",
        "def stemming_on_text(data):\n",
        "    text = [st.stem(word) for word in data]\n",
        "    return data\n",
        "\n",
        "data['text']= data['text'].apply(lambda x: stemming_on_text(x))\n",
        "data['text'].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "799999             [love, healthuandpets, u, guys, r, best]\n",
              "800000    [im, meeting, one, besties, tonight, cant, wai...\n",
              "800001    [darealsunisakim, thanks, twitter, add, sunisa...\n",
              "800002    [sick, really, cheap, hurts, much, eat, real, ...\n",
              "800003                    [lovesbrooklyn, effect, everyone]\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98bZskfPhWKP"
      },
      "source": [
        "Applying Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s74TZyuohW2E",
        "outputId": "51a99096-e9db-4edf-fce5-e0c1913ddb57"
      },
      "source": [
        "lm = nltk.WordNetLemmatizer()\n",
        "def lemmatizer_on_text(data):\n",
        "    text = [lm.lemmatize(word) for word in data]\n",
        "    return data\n",
        "\n",
        "data['text'] = data['text'].apply(lambda x: lemmatizer_on_text(x))\n",
        "data['text'].head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "799999             [love, healthuandpets, u, guys, r, best]\n",
              "800000    [im, meeting, one, besties, tonight, cant, wai...\n",
              "800001    [darealsunisakim, thanks, twitter, add, sunisa...\n",
              "800002    [sick, really, cheap, hurts, much, eat, real, ...\n",
              "800003                    [lovesbrooklyn, effect, everyone]\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjzchkR5jM6C"
      },
      "source": [
        "Data splitting and model building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCuxcP2EjMVO",
        "outputId": "601dd09e-fe65-4b6c-e638-e7337ead0322"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "X=data.text\n",
        "y=data.label\n",
        "\n",
        "max_len = 500\n",
        "tok = Tokenizer(num_words=2000)\n",
        "tok.fit_on_texts(X)\n",
        "sequences = tok.texts_to_sequences(X)\n",
        "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
        "\n",
        "print(sequences_matrix.shape)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(sequences_matrix, y, test_size=0.3, random_state=2)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(160000, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxqhEEpkBJbJ"
      },
      "source": [
        "# TENSORFLOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hze5zlKKBMjv"
      },
      "source": [
        "def tensorflow_based_model():\n",
        "    inputs = Input(name='inputs',shape=[max_len])\n",
        "    layer = Embedding(2000,50,input_length=max_len)(inputs)\n",
        "    layer = LSTM(64)(layer)\n",
        "    layer = Dense(256,name='FC1')(layer)\n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dropout(0.5)(layer)\n",
        "    layer = Dense(1,name='out_layer')(layer)\n",
        "    layer = Activation('sigmoid')(layer)\n",
        "    model = Model(inputs=inputs,outputs=layer) \n",
        "    return model"
      ],
      "execution_count": 34,
      "outputs": []
    }
  ]
}